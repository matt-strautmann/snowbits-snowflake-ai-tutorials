{
 "metadata": {
  "kernelspec": {
   "display_name": "snowpark-container-services-hol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000000",
   "metadata": {
    "collapsed": false,
    "name": "Intro"
   },
   "source": "# Data Exploration using Snowflake's Native Pandas -- Modin\n\nThis notebook will explore a dataset loaded into Snowflake, leveraging **Snowpark \"Modin\" Dataframes** to natively handle large datasets efficiently and directly within the Snowflake environment. This keeps all operations scalable and optimized for Snowflake's architecture while enabling distributed processing. We will also perform standard feature engineering tasks.\n\n### Key Objectives:\n- Inspect the dataset (schema, missing values, basic statistics)\n- Explore numerical and categorical features\n- Visualize a subset of the data"
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000001",
   "metadata": {
    "collapsed": false,
    "name": "S_0__Import_And_Session"
   },
   "source": [
    "## 0 - Import Packages and Get Active Snowflake Session Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000002",
   "metadata": {
    "language": "python",
    "name": "PACKAGE_IMPORTS"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "\n",
    "import snowflake.snowpark.functions as F\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "import modin.pandas as spd  # Use Modin's Snowpark Pandas version\n",
    "import snowflake.snowpark.modin.plugin  # Plugin to connect Modin with Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000003",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "CREATE_SESSION"
   },
   "outputs": [],
   "source": "## Session Initialization\n#   Snowflake Notebooks automatically manage sessions, so we don't need to set up the connection manually.\n#   We'll retrieve the active session using 'get_active_session()'.\n#   We are using an x-small warehouse for this demo.\n\nsession = get_active_session()"
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000004",
   "metadata": {
    "collapsed": false,
    "name": "S_1__Load_Data"
   },
   "source": [
    "## 1 - Load Data Into Snowflake for Exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000005",
   "metadata": {
    "collapsed": false,
    "name": "S1_1__Create_Dataframes"
   },
   "source": [
    "### 1.1 Load Data into Snowflake Pandas Dataframes\n",
    "\n",
    "Loading Data into Snowflake using Snowflake Modin Pandas Plugin\n",
    "In this notebook, we will use the **Snowflake Snowpark Modin Pandas plugin** to load large CSV files into Snowflake. This plugin allows efficient data loading and processing in parallel using Modin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000006",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "IMPORT_CSVS_INTO_DFS"
   },
   "outputs": [],
   "source": "# Load the CSV files using Snowpark's Modin Pandas plugin\napplication_record_df = spd.read_csv('data/application_record.csv.zip')\ncredit_record_df = spd.read_csv('data/credit_record.csv.zip')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000007",
   "metadata": {
    "language": "python",
    "name": "APPLICATION_RECORD_DF",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "# Display the first few rows\n",
    "application_record_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000008",
   "metadata": {
    "language": "python",
    "name": "CREDIT_REPORT_DF"
   },
   "outputs": [],
   "source": [
    "# Display the last few rows\n",
    "credit_record_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000009",
   "metadata": {
    "collapsed": false,
    "name": "S_1_2__Upload"
   },
   "source": [
    "### 1.2 Upload Data to Snowflake\n",
    "\n",
    "Now that the data is loaded, we can upload it directly to Snowflake using the `write_pandas` method.\n",
    "This creates the necessary tables and uploads the data to Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000010",
   "metadata": {
    "language": "python",
    "name": "UPLOAD_TO_TABLES"
   },
   "outputs": [],
   "source": [
    "# Upload the DataFrames to Snowflake tables\n",
    "session.write_pandas(application_record_df, table_name='APPLICATION_RECORD', auto_create_table=True, overwrite=True)\n",
    "session.write_pandas(credit_record_df, table_name='CREDIT_RECORD', auto_create_table=True, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000011",
   "metadata": {
    "collapsed": false,
    "name": "S_2__Data_Structure"
   },
   "source": [
    "## 2 - Data Structure Exploration\n",
    "\n",
    "We will begin with some basic transformations and analysis on 'application_record_df'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f50fb8-9423-4166-8ead-9d8006638844",
   "metadata": {
    "collapsed": false,
    "name": "S_2_1__Basic_Transformations"
   },
   "source": [
    "### 2.1 - Basic Transformations\n",
    "\n",
    "1. Convert age from days to years\n",
    "2. Find average age per gender and income type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000012",
   "metadata": {
    "language": "python",
    "name": "CREATE_AGE_IN_YEARS"
   },
   "outputs": [],
   "source": [
    "# Convert DAYS_BIRTH to numeric (if needed)\n",
    "application_record_df['DAYS_BIRTH'] = spd.to_numeric(application_record_df['DAYS_BIRTH'], errors='coerce')\n",
    "\n",
    "# Now create the AGE column and ensure it's an integer\n",
    "application_record_df['AGE'] = application_record_df['DAYS_BIRTH'].apply(lambda x: np.floor(abs(x) / 365)).astype(int)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(application_record_df[['DAYS_BIRTH', 'AGE']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000014",
   "metadata": {
    "collapsed": false,
    "name": "S_2_2__Data_Exploration"
   },
   "source": [
    "### 2.2 - Data Exploration\n",
    "\n",
    "1. Count Total Rows\n",
    "2. Drop Duplicates\n",
    "3. Get Summary Stats for Numerical Columns\n",
    "4. Schema Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000015",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "COUNT_TOTAL_ROWS"
   },
   "outputs": [],
   "source": [
    "## 1. Count Total Rows\n",
    "# Count the number of rows in application_record_df\n",
    "row_count = application_record_df.count()\n",
    "\n",
    "print(f\"Total number of rows: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000016",
   "metadata": {
    "language": "python",
    "name": "DROP_DUPLICATES"
   },
   "outputs": [],
   "source": [
    "## 2. Drop Duplicates\n",
    "# Drop duplicates based on the 'ID' column\n",
    "application_record_df = application_record_df.drop_duplicates('ID')\n",
    "\n",
    "print(f\"Number of rows after dropping duplicates: {application_record_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000017",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "NUMERIC_COLUMNS"
   },
   "outputs": [],
   "source": [
    "## 3. Get Summary Stats for Numerical Columns\n",
    "# We can generate descriptive statistics for numerical columns to understand the data distribution.\n",
    "\n",
    "# Identify numerical columns (integer and float types)\n",
    "numerical_columns = application_record_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "print(f\"Numerical columns: \\n {numerical_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000018",
   "metadata": {
    "language": "python",
    "name": "SCHEMA_EXPLORATION"
   },
   "outputs": [],
   "source": [
    "## 4. Schema Exploration\n",
    "# We start by inspecting the schema to identify the data types of each column. \n",
    "# This exploration involves schema inspection, identifying categorical and numerical columns, and summarizing the distribution of income by gender and income type.\n",
    "\n",
    "# Show the schema of the DataFrame\n",
    "print(application_record_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000019",
   "metadata": {
    "language": "python",
    "name": "CATEGORICAL_COLUMNS"
   },
   "outputs": [],
   "source": [
    "# Categorical and Numerical Features\n",
    "# Now, letâ€™s identify which columns are categorical and which are numerical. \n",
    "\n",
    "# Identify categorical columns (string types)\n",
    "categorical_columns = application_record_df.select_dtypes(include='object').columns.tolist()\n",
    "print(f\"Categorical columns: \\n {categorical_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000021",
   "metadata": {
    "language": "python",
    "name": "GET_UNIQUE_CATEGORIES"
   },
   "outputs": [],
   "source": [
    "# Unique Values in Categorical Columns\n",
    "# To understand the diversity in the categorical data, we will compute the number of unique values in each categorical column.\n",
    "# Calculate the number of unique values per categorical column\n",
    "unique_values = [(col, application_record_df[col].nunique()) for col in categorical_columns]\n",
    "\n",
    "# Convert to DataFrame and display the results\n",
    "unique_values_df = spd.DataFrame(unique_values, columns=['COLUMN_NAME', 'NUM_UNIQUE_VALUES'])\n",
    "print(unique_values_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000022",
   "metadata": {
    "language": "python",
    "name": "AVG_INCOME_PER_INCOME_TYPE"
   },
   "outputs": [],
   "source": "\n# Distribution of Income by Income Type\n# Next, we will compute the average income grouped by NAME_INCOME_TYPE. \n# Using groupby and agg, this operation is distributed and faster for large datasets.\n\n# Group by 'NAME_INCOME_TYPE' and 'CODE_GENDER', calculate the average income\nanalysis_df = application_record_df.groupby('NAME_INCOME_TYPE').agg({'AMT_INCOME_TOTAL': 'mean'}).reset_index()\n\n# Rename the column for better readability\nanalysis_df.columns = ['NAME_INCOME_TYPE', 'AVG_INCOME']\n\n# Sort the results by 'NAME_INCOME_TYPE' (ascending) and 'AVG_INCOME' (descending)\nanalysis_df = analysis_df.sort_values(by=['NAME_INCOME_TYPE', 'AVG_INCOME'], ascending=[True, False])\n\n\n# Display the final result!\nprint(analysis_df)"
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000023",
   "metadata": {
    "name": "S_3__Presist_Transformations",
    "collapsed": false
   },
   "source": [
    "## 3 - Persist Transformations\n",
    "If we want to save the changes we can either save it as a table, meaning the SQL generated by the DataFrame is executed and the result is stored in a table or as a view where the DataFrame SQL will be the definition of the view.\n",
    "save_as_table saves the result in a table, if mode='overwrite' then it will also replace the data that is in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000024",
   "metadata": {
    "language": "python",
    "name": "WRITE_TO_SNOWFLAKE"
   },
   "outputs": [],
   "source": [
    "session.write_pandas(application_record_df, table_name='APPLICATION_RECORD', auto_create_table=True, overwrite=True)\n",
    "session.table('APPLICATION_RECORD').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9268e1-fed6-4e33-99b9-f6c9c5c21a36",
   "metadata": {
    "language": "sql",
    "name": "REVIEW_TABLE_IN_SNOWFLAKE"
   },
   "outputs": [],
   "source": [
    "select *\n",
    "from application_record\n",
    "fetch 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03cc0cd-f7f4-43c8-a109-89d3a3cbea41",
   "metadata": {
    "language": "python",
    "name": "CLOSE_SESSION"
   },
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc355e2-b36a-443b-abb0-c6812513c87b",
   "metadata": {
    "collapsed": false,
    "name": "S_4__Conclusion"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "The goal of this notebook is to help you understand how to use Pandas natively in Snowflake Snowpark. By integrating Pandas with Snowpark, you can leverage powerful data manipulation and analysis tools directly within your Snowflake environment, enhancing your data workflows and efficiency.\n",
    "\n",
    "If you are interested in more guides, please consider:\n",
    "\n",
    "- Following me on [LinkedIn](https://www.linkedin.com/in/mattstrautmann/) for updates, tutorials, and professional insights.\n",
    "- Subscribing to my [Snowbits AI Kickstart blog](https://mattstrautmann.substack.com/?utm_source=github&utm_medium=web&utm_campaign=substack_profile) for in-depth articles and the latest trends in data analytics and AI.\n",
    "\n",
    "\n",
    "\n",
    "*Thank you for exploring this notebook!*"
   ]
  }
 ]
}