{
 "metadata": {
  "kernelspec": {
   "display_name": "snowpark-container-services-hol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell1"
   },
   "source": [
    "# Data Exploration and Feature Engineering with Snowpark\n",
    "\n",
    "In this notebook, we will explore a dataset loaded into Snowflake, leveraging **Snowpark** to handle large datasets efficiently and directly within the Snowflake environment. This keeps all operations scalable and optimized for Snowflake's architecture, while also enabling distributed processing. We will also perform standard feature engineering tasks.\n",
    "\n",
    "### Key Objectives:\n",
    "1. Inspect the dataset (schema, missing values, basic statistics)\n",
    "2. Explore numerical and categorical features\n",
    "3. Perform feature engineering\n",
    "4. Visualize a subset of the data"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000000"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell2"
   },
   "source": [
    "## 0 - Import Packages and Get Active Snowflake Session Object"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000001"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell3",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "from snowflake.snowpark.session import get_active_session\n",
    "\n",
    "# Import the Snowpark pandas plugin for Modin\n",
    "import modin.pandas as spd  # Use Modin's Snowpark Pandas version\n",
    "import snowflake.snowpark.modin.plugin  # Plugin to connect Modin with Snowflake\n",
    "import snowflake.snowpark.functions as F\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000002"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell4",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "## Session Initialization\n",
    "#   Snowflake Notebooks automatically manage sessions, so we don't need to manually set up the connection.\n",
    "#   We'll retrieve the active session using `get_active_session()`.\n",
    "\n",
    "session = get_active_session()"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000003"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell5"
   },
   "source": [
    "## 1 - Load Data Into Snowflake for Exploration\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000004"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell6"
   },
   "source": [
    "### 1.1 Load Data into Snowflake Pandas Dataframes\n",
    "\n",
    "Loading Data into Snowflake using Snowflake Modin Pandas Plugin\n",
    "In this notebook, we will use the **Snowflake Snowpark Modin Pandas plugin** to load large CSV files into Snowflake. This plugin allows efficient data loading and processing in parallel using Modin."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000005"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell7",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Load the CSV files using Snowpark's Modin Pandas plugin\n",
    "application_record_df = spd.read_csv('data/application_record.csv.zip')\n",
    "credit_record_df = spd.read_csv('data/credit_record.csv.zip')"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000006"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell8",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Display the first few rows\n",
    "application_record_df.limit(5).show()"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000007"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell9",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Display the first few rows\n",
    "credit_record_df.limit(5).show()"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000008"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell10"
   },
   "source": [
    "### 1.2 Upload Data to Snowflake\n",
    "\n",
    "Now that the data is loaded, we can upload it directly to Snowflake using the `write_pandas` method.\n",
    "This creates the necessary tables and uploads the data to Snowflake."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000009"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell11",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Upload the Modin DataFrames to Snowflake tables\n",
    "session.write_pandas(application_record_df, table_name='APPLICATION_RECORD', auto_create_table=True, overwrite=True)\n",
    "session.write_pandas(credit_record_df, table_name='CREDIT_RECORD', auto_create_table=True, overwrite=True)"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000010"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell12"
   },
   "source": [
    "## 2 - Data Structure Exploration\n",
    "\n",
    "We will begin with some basic transformations and analysis on two dataframes: application_record_df."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000011"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell13",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Create a new AGE column based on DAYS_BIRTH\n",
    "application_record_df = application_record_df.with_column('AGE', F.floor(F.abs(F.col('DAYS_BIRTH')) / 365))\n",
    "\n",
    "# Drop the old DAYS_BIRTH column\n",
    "application_record_df = application_record_df.drop('DAYS_BIRTH')\n",
    "\n",
    "application_record_df.show()"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000012"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell14",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Filter rows based on income type\n",
    "application_record_df = application_record_df.filter(F.col('NAME_INCOME_TYPE').in_(['Pensioner', 'Student']))\n",
    "\n",
    "# Group and Aggregate Data\n",
    "# We’ll group the data by CODE_GENDER and NAME_INCOME_TYPE and calculate the average age.\n",
    "application_record_df = application_record_df.group_by(['CODE_GENDER', 'NAME_INCOME_TYPE']).agg(F.avg('AGE').alias('AVG_AGE'))\n",
    "\n",
    "# Sort the data by AVG_AGE in descending order\n",
    "application_record_df = application_record_df.sort(col('AVG_AGE').desc())\n",
    "\n",
    "application_record_df.show()"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000013"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell15"
   },
   "source": [
    "### 2.1 - Data Exploration\n",
    "\n",
    "1. Count Total Rows\n",
    "2. Drop Duplicates\n",
    "3. Get Summary Stats for Numerical Columns\n",
    "4. Schema Exploration"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000014"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell16",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "## 1. Count Total Rows\n",
    "# Count the number of rows in application_record_df\n",
    "row_count = application_record_df.count()\n",
    "\n",
    "print(f\"Total number of rows: {row_count}\")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000015"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell17",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "## 2. Drop Duplicates\n",
    "# Drop duplicates based on the 'ID' column\n",
    "application_record_df = application_record_df.drop_duplicates('ID')\n",
    "\n",
    "print(f\"Number of rows after dropping duplicates: {application_record_df.count()}\")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000016"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell18",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "## 3. Get Summary Stats for Numerical Columns\n",
    "# We can generate descriptive statistics for numerical columns to understand the data distribution.\n",
    "\n",
    "# Get summary statistics for numerical columns\n",
    "application_record_df.describe().show()"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000017"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell19",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "## 4. Schema Exploration\n",
    "# We start by inspecting the schema to identify the data types of each column. \n",
    "# This exploration involves schema inspection, identifying categorical and numerical columns, and summarizing the distribution of income by gender and income type.\n",
    "\n",
    "# Show the schema of the DataFrame\n",
    "print(application_record_df.dtypes)"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000018"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell20",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Categorical and Numerical Features\n",
    "# Now, let’s identify which columns are categorical and which are numerical. \n",
    "\n",
    "# Identify categorical columns (string types)\n",
    "categorical_columns = application_record_df.select_dtypes(include='object').columns.tolist()\n",
    "print(f\"Categorical columns: {categorical_columns}\")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000019"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell21",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Identify numerical columns (integer and float types)\n",
    "numerical_columns = application_record_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "print(f\"Numerical columns: {numerical_columns}\")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000020"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell22",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Unique Values in Categorical Columns\n",
    "# To understand the diversity in the categorical data, we will compute the number of unique values in each categorical column.\n",
    "# Calculate the number of unique values per categorical column\n",
    "unique_values = [(col, application_record_df[col].nunique()) for col in categorical_columns]\n",
    "\n",
    "# Convert to DataFrame and display the results\n",
    "unique_values_df = spd.DataFrame(unique_values, columns=['COLUMN_NAME', 'NUM_UNIQUE_VALUES'])\n",
    "print(unique_values_df)"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000021"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell23",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Distribution of Income by Gender and Income Type\n",
    "# Next, we will compute the average income grouped by NAME_INCOME_TYPE and CODE_GENDER. \n",
    "# Using groupby and agg, this operation is distributed and faster for large datasets.\n",
    "\n",
    "# Group by 'NAME_INCOME_TYPE' and 'CODE_GENDER', calculate the average income\n",
    "analysis_df = application_record_df.groupby(['NAME_INCOME_TYPE', 'CODE_GENDER']).agg({'AMT_INCOME_TOTAL': 'mean'}).reset_index()\n",
    "\n",
    "# Rename the column for better readability\n",
    "analysis_df.columns = ['NAME_INCOME_TYPE', 'CODE_GENDER', 'AVG_INCOME']\n",
    "\n",
    "# Sort the results by 'NAME_INCOME_TYPE' (ascending) and 'AVG_INCOME' (descending)\n",
    "analysis_df = analysis_df.sort_values(by=['NAME_INCOME_TYPE', 'AVG_INCOME'], ascending=[True, False])\n",
    "\n",
    "\n",
    "# Display the final result!\n",
    "print(analysis_df)"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000022"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell24"
   },
   "source": [
    "## 3 - Persist Transformations\n",
    "If we want to save the changes we can either save it as a table, meaning the SQL generated by the DataFrame is executed and the result is stored in a table or as a view where the DataFrame SQL will be the definition of the view.\n",
    "save_as_table saves the result in a table, if mode='overwrite' then it will also replace the data that is in it."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000023"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell25",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "application_record_df.write.save_as_table(table_name='APPLICATION_RECORD', mode='overwrite')\n",
    "session.table('APPLICATION_RECORD').show()"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000024"
  }
 ]
}