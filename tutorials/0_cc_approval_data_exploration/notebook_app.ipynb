{
 "metadata": {
  "kernelspec": {
   "display_name": "snowpark-container-services-hol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Intro",
    "collapsed": false
   },
   "source": "# Data Exploration and Feature Engineering with Snowpark\n\nIn this notebook, we will explore a dataset loaded into Snowflake, leveraging **Snowpark \"Modin\" Dataframes** to natively handle large datasets efficiently and directly within the Snowflake environment. This keeps all operations scalable and optimized for Snowflake's architecture, while also enabling distributed processing. We will also perform standard feature engineering tasks.\n\n### Key Objectives:\n1. Inspect the dataset (schema, missing values, basic statistics)\n2. Explore numerical and categorical features\n3. Perform feature engineering\n4. Visualize a subset of the data",
   "id": "ce110000-1111-2222-3333-ffffff000000"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "S_0__Import_And_Session",
    "collapsed": false
   },
   "source": [
    "## 0 - Import Packages and Get Active Snowflake Session Object"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000001"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "PACKAGE_IMPORTS",
    "language": "python"
   },
   "outputs": [],
   "source": "# Import required libraries\nimport numpy as np\n\nimport snowflake.snowpark.functions as F\nfrom snowflake.snowpark.context import get_active_session\nimport modin.pandas as spd  # Use Modin's Snowpark Pandas version\nimport snowflake.snowpark.modin.plugin  # Plugin to connect Modin with Snowflake",
   "id": "ce110000-1111-2222-3333-ffffff000002"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "CREATE_SESSION",
    "language": "python",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "## Session Initialization\n#   Snowflake Notebooks automatically manage sessions, so we don't need to manually set up the connection.\n#   We'll retrieve the active session using 'get_active_session()'.\n#   We are using an x-small warehouse for this demo.\n\nsession = get_active_session()",
   "id": "ce110000-1111-2222-3333-ffffff000003"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "S_1__Load_Data",
    "collapsed": false
   },
   "source": [
    "## 1 - Load Data Into Snowflake for Exploration\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000004"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "S1_1__Create_Dataframes",
    "collapsed": false
   },
   "source": [
    "### 1.1 Load Data into Snowflake Pandas Dataframes\n",
    "\n",
    "Loading Data into Snowflake using Snowflake Modin Pandas Plugin\n",
    "In this notebook, we will use the **Snowflake Snowpark Modin Pandas plugin** to load large CSV files into Snowflake. This plugin allows efficient data loading and processing in parallel using Modin."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000005"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "IMPORT_CSVS_INTO_DFS",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Load the CSV files using Snowpark's Modin Pandas plugin\napplication_record_df = spd.read_csv('0_cc_approval_data_exploration/data/application_record.csv.zip')\ncredit_record_df = spd.read_csv('0_cc_approval_data_exploration/data/credit_record.csv.zip')",
   "id": "ce110000-1111-2222-3333-ffffff000006"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "APPLICATION_RECORD_DF",
    "language": "python"
   },
   "outputs": [],
   "source": "# Display the first few rows\napplication_record_df.describe()",
   "id": "ce110000-1111-2222-3333-ffffff000007"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "CREDIT_REPORT_DF",
    "language": "python"
   },
   "outputs": [],
   "source": "# Display the last few rows\ncredit_record_df.describe()",
   "id": "ce110000-1111-2222-3333-ffffff000008"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "S_1_2__Upload",
    "collapsed": false
   },
   "source": [
    "### 1.2 Upload Data to Snowflake\n",
    "\n",
    "Now that the data is loaded, we can upload it directly to Snowflake using the `write_pandas` method.\n",
    "This creates the necessary tables and uploads the data to Snowflake."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000009"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "UPLOAD_TO_TABLES",
    "language": "python"
   },
   "outputs": [],
   "source": "# Upload the DataFrames to Snowflake tables\nsession.write_pandas(application_record_df, table_name='APPLICATION_RECORD', auto_create_table=True, overwrite=True)\nsession.write_pandas(credit_record_df, table_name='CREDIT_RECORD', auto_create_table=True, overwrite=True)",
   "id": "ce110000-1111-2222-3333-ffffff000010"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "S_2__Data_Structure",
    "collapsed": false
   },
   "source": "## 2 - Data Structure Exploration\n\nWe will begin with some basic transformations and analysis on 'application_record_df'.",
   "id": "ce110000-1111-2222-3333-ffffff000011"
  },
  {
   "cell_type": "markdown",
   "id": "10f50fb8-9423-4166-8ead-9d8006638844",
   "metadata": {
    "name": "S_2_1__Basic_Transformations",
    "collapsed": false
   },
   "source": "### 2.1 - Basic Transformations\n\n1. Convert age from days to years\n2. Find average age per gender and income type."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "CREATE_AGE_IN_YEARS",
    "language": "python"
   },
   "outputs": [],
   "source": "# Convert DAYS_BIRTH to numeric (if needed)\napplication_record_df['DAYS_BIRTH'] = spd.to_numeric(application_record_df['DAYS_BIRTH'], errors='coerce')\n\n# Now create the AGE column and ensure it's an integer\napplication_record_df['AGE'] = application_record_df['DAYS_BIRTH'].apply(lambda x: np.floor(abs(x) / 365)).astype(int)\n\n# Display the first few rows to verify\nprint(application_record_df[['DAYS_BIRTH', 'AGE']].head())",
   "id": "ce110000-1111-2222-3333-ffffff000012"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "FIND_AVG_AGE_PER_GENDER_INCOME_TYPE",
    "language": "python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Group the data by 'CODE_GENDER' and 'NAME_INCOME_TYPE', calculate the average 'AGE'\ngrouped_df = application_record_df.groupby(['CODE_GENDER', 'NAME_INCOME_TYPE'])['AGE'].mean().reset_index()\n\n# Rename the aggregated column for clarity\ngrouped_df.rename(columns={'AGE': 'AVG_AGE'}, inplace=True)\n\n# Merge the grouped result back into the original DataFrame\napplication_record_df = application_record_df.merge(grouped_df, on=['CODE_GENDER', 'NAME_INCOME_TYPE'], how='left')\n\n# Display the updated DataFrame with the new 'AVG_AGE' column\napplication_record_df[['CODE_GENDER', 'NAME_INCOME_TYPE', 'AGE', 'AVG_AGE']].head()",
   "id": "ce110000-1111-2222-3333-ffffff000013"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "S_2_2__Data_Exploration",
    "collapsed": false
   },
   "source": "### 2.2 - Data Exploration\n\n1. Count Total Rows\n2. Drop Duplicates\n3. Get Summary Stats for Numerical Columns\n4. Schema Exploration",
   "id": "ce110000-1111-2222-3333-ffffff000014"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "COUNT_TOTAL_ROWS",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 1. Count Total Rows\n",
    "# Count the number of rows in application_record_df\n",
    "row_count = application_record_df.count()\n",
    "\n",
    "print(f\"Total number of rows: {row_count}\")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000015"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "DROP_DUPLICATES",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "## 2. Drop Duplicates\n",
    "# Drop duplicates based on the 'ID' column\n",
    "application_record_df = application_record_df.drop_duplicates('ID')\n",
    "\n",
    "print(f\"Number of rows after dropping duplicates: {application_record_df.count()}\")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000016"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "NUMERIC_COLUMNS",
    "language": "python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "## 3. Get Summary Stats for Numerical Columns\n# We can generate descriptive statistics for numerical columns to understand the data distribution.\n\n# Identify numerical columns (integer and float types)\nnumerical_columns = application_record_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\nprint(f\"Numerical columns: \\n {numerical_columns}\")",
   "id": "ce110000-1111-2222-3333-ffffff000017"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "SCHEMA_EXPLORATION",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "## 4. Schema Exploration\n",
    "# We start by inspecting the schema to identify the data types of each column. \n",
    "# This exploration involves schema inspection, identifying categorical and numerical columns, and summarizing the distribution of income by gender and income type.\n",
    "\n",
    "# Show the schema of the DataFrame\n",
    "print(application_record_df.dtypes)"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000018"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "CATEGORICAL_COLUMNS",
    "language": "python"
   },
   "outputs": [],
   "source": "# Categorical and Numerical Features\n# Now, let’s identify which columns are categorical and which are numerical. \n\n# Identify categorical columns (string types)\ncategorical_columns = application_record_df.select_dtypes(include='object').columns.tolist()\nprint(f\"Categorical columns: \\n {categorical_columns}\")",
   "id": "ce110000-1111-2222-3333-ffffff000019"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "GET_UNIQUE_CATEGORIES",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Unique Values in Categorical Columns\n",
    "# To understand the diversity in the categorical data, we will compute the number of unique values in each categorical column.\n",
    "# Calculate the number of unique values per categorical column\n",
    "unique_values = [(col, application_record_df[col].nunique()) for col in categorical_columns]\n",
    "\n",
    "# Convert to DataFrame and display the results\n",
    "unique_values_df = spd.DataFrame(unique_values, columns=['COLUMN_NAME', 'NUM_UNIQUE_VALUES'])\n",
    "print(unique_values_df)"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000021"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "AVG_INCOME_PER_INCOME_TYPE_GENDER",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Distribution of Income by Gender and Income Type\n",
    "# Next, we will compute the average income grouped by NAME_INCOME_TYPE and CODE_GENDER. \n",
    "# Using groupby and agg, this operation is distributed and faster for large datasets.\n",
    "\n",
    "# Group by 'NAME_INCOME_TYPE' and 'CODE_GENDER', calculate the average income\n",
    "analysis_df = application_record_df.groupby(['NAME_INCOME_TYPE', 'CODE_GENDER']).agg({'AMT_INCOME_TOTAL': 'mean'}).reset_index()\n",
    "\n",
    "# Rename the column for better readability\n",
    "analysis_df.columns = ['NAME_INCOME_TYPE', 'CODE_GENDER', 'AVG_INCOME']\n",
    "\n",
    "# Sort the results by 'NAME_INCOME_TYPE' (ascending) and 'AVG_INCOME' (descending)\n",
    "analysis_df = analysis_df.sort_values(by=['NAME_INCOME_TYPE', 'AVG_INCOME'], ascending=[True, False])\n",
    "\n",
    "\n",
    "# Display the final result!\n",
    "print(analysis_df)"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000022"
  },
  {
   "cell_type": "markdown",
   "id": "de16fb0e-e49e-47b2-931f-a6f5105f8219",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": ""
  },
  {
   "cell_type": "code",
   "id": "43390dfb-a901-4c6a-a212-b7ea12aca907",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": "my_imputer = SimpleImputer(input_cols=['OCCUPATION_TYPE'], output_cols=['OCCUPATION_TYPE'] ,strategy='most_frequent')\nmy_imputer.fit(snowpark_df)\nsnowpark_df = my_imputer.transform(snowpark_df)\nsnowpark_df.describe().show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell24"
   },
   "source": [
    "## 3 - Persist Transformations\n",
    "If we want to save the changes we can either save it as a table, meaning the SQL generated by the DataFrame is executed and the result is stored in a table or as a view where the DataFrame SQL will be the definition of the view.\n",
    "save_as_table saves the result in a table, if mode='overwrite' then it will also replace the data that is in it."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000023"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "WRITE_TO_SNOWFLAKE",
    "language": "python"
   },
   "outputs": [],
   "source": "session.write_pandas(application_record_df, table_name='APPLICATION_RECORD', auto_create_table=True, overwrite=True)\nsession.table('APPLICATION_RECORD').show()",
   "id": "ce110000-1111-2222-3333-ffffff000024"
  }
 ]
}