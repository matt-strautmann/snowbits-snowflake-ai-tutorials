{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# Snowflake RAG Pipeline Tutorial\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this tutorial, we'll build a basic Retrieval-Augmented Generation (RAG) pipeline using Snowflake, LlamaIndex, and Snowflake Cortex within a Snowflake native notebook. We will handle unstructured text data stored in Snowflake, preprocess it using Snowflake Cortex's new SQL functions `PARSE_DOCUMENT` and `SPLIT_TEXT_RECURSIVE_CHARACTER`, build an index using LlamaIndex, and generate intelligent, context-aware responses.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Access to a Snowflake account with permissions to create databases, schemas, and execute Python code.\n",
    "- Familiarity with Snowflake's native notebook in Snowsight.\n",
    "- Access to Snowflake Cortex and a deployed language model.\n",
    "- LlamaIndex library available in your environment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steps-overview",
   "metadata": {},
   "source": [
    "## Steps Overview\n",
    "\n",
    "1. **Data Preparation**: Load and preprocess documents using Snowflake Cortex SQL functions.\n",
    "2. **Index Building**: Use LlamaIndex to build an index over the preprocessed data.\n",
    "3. **Setting Up Cortex LLM**: Utilize Snowflake Cortex to interact with a language model.\n",
    "4. **Querying the Index**: Perform queries to retrieve augmented responses.\n",
    "5. **Before and After Comparison**: Show raw input vs. RAG output.\n",
    "6. **Cortex Use Cases**: Demonstrate simple use cases with Cortex.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-preparation",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "### 1.1 Load Documents into Snowflake\n",
    "\n",
    "First, we'll create a table to store the documents. We'll assume the documents are stored in an external stage (e.g., AWS S3) and accessible to Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-document-files-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Create a table to store document metadata\n",
    "CREATE OR REPLACE TABLE document_files (\n",
    "    id INT AUTOINCREMENT,\n",
    "    file_name STRING,\n",
    "    file_url STRING\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insert-document-metadata",
   "metadata": {},
   "source": [
    "Insert the document metadata into the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insert-document-files",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Insert document metadata (replace with your actual file URLs)\n",
    "INSERT INTO document_files (file_name, file_url) VALUES\n",
    "('document1.pdf', 's3://your-bucket/document1.pdf'),\n",
    "('document2.pdf', 's3://your-bucket/document2.pdf'),\n",
    "('document3.pdf', 's3://your-bucket/document3.pdf');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extract-text-using-parse-document",
   "metadata": {},
   "source": [
    "### 1.2 Extract Text Using PARSE_DOCUMENT\n",
    "\n",
    "Use the `PARSE_DOCUMENT` function to extract text from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-document-texts-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Create a table to store extracted text\n",
    "CREATE OR REPLACE TABLE document_texts AS\n",
    "SELECT\n",
    "    id,\n",
    "    PARSE_DOCUMENT(\n",
    "        FILE => file_url,\n",
    "        FILE_TYPE => 'PDF',\n",
    "        CONTENT => 'TEXT',\n",
    "        PARSE_STRATEGY => 'LAYOUT'\n",
    "    ) AS content\n",
    "FROM document_files;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-text-using-split-text-recursive-character",
   "metadata": {},
   "source": [
    "This function reads the PDF files, extracts the text content while preserving the layout, and stores it in the `content` column.\n",
    "\n",
    "### 1.3 Split Text Using SPLIT_TEXT_RECURSIVE_CHARACTER\n",
    "\n",
    "Now, we'll split the extracted text into smaller chunks suitable for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-document-chunks-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Create a table to store text chunks\n",
    "CREATE OR REPLACE TABLE document_chunks AS\n",
    "SELECT\n",
    "    id,\n",
    "    SEQ4() OVER (PARTITION BY id ORDER BY SEQ4()) AS chunk_id,\n",
    "    value::STRING AS chunk_text\n",
    "FROM (\n",
    "    SELECT\n",
    "        id,\n",
    "        SPLIT_TEXT_RECURSIVE_CHARACTER(\n",
    "            TEXT => content,\n",
    "            CHUNK_LENGTH => 500,\n",
    "            CHUNK_OVERLAP => 50\n",
    "        ) AS chunks\n",
    "    FROM document_texts\n",
    "), LATERAL FLATTEN(input => chunks);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "index-building-with-llamaindex",
   "metadata": {},
   "source": [
    "This function splits the text into chunks of approximately 500 characters with an overlap of 50 characters between chunks.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Index Building with LlamaIndex\n",
    "\n",
    "### 2.1 Import Libraries\n",
    "\n",
    "In the first Python cell of your notebook, import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark as snowpark\n",
    "from snowflake.snowpark.session import Session\n",
    "from llama_index import (\n",
    "    LLMPredictor,\n",
    "    ServiceContext,\n",
    "    GPTVectorStoreIndex\n",
    ")\n",
    "from llama_index.data_structs import Node\n",
    "from llama_index.embeddings.base import BaseEmbedding\n",
    "from llama_index.vector_stores import SimpleVectorStore\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "use-the-active-snowflake-session",
   "metadata": {},
   "source": [
    "### 2.2 Use the Active Snowflake Session\n",
    "\n",
    "In the Snowflake native notebook environment, you can use the active session without creating a new connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get-active-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the active Snowflake session\n",
    "session = snowpark.Session.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "define-a-custom-embedding-class-using-cortex",
   "metadata": {},
   "source": [
    "### 2.3 Define a Custom Embedding Class Using Cortex\n",
    "\n",
    "We'll define a custom embedding class that uses Snowflake Cortex's embedding capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-cortexembedding-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CortexEmbedding(BaseEmbedding):\n",
    "    def __init__(self, session):\n",
    "        self.session = session\n",
    "\n",
    "    def get_text_embedding(self, text: str) -> List[float]:\n",
    "        df = self.session.create_dataframe([[text]], schema=[\"TEXT\"])\n",
    "        result_df = df.select(\n",
    "            self.session.call_function(\"CORTEX_EMBED\", df[\"TEXT\"]).alias(\"EMBEDDING\")\n",
    "        )\n",
    "        result = result_df.collect()\n",
    "        embedding = result[0][\"EMBEDDING\"]\n",
    "        return embedding\n",
    "\n",
    "    async def aget_text_embedding(self, text: str) -> List[float]:\n",
    "        return self.get_text_embedding(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "note-on-cortex-embed-function",
   "metadata": {},
   "source": [
    "**Note**: You'll need to have the `CORTEX_EMBED` function available in your Snowflake environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build-the-index",
   "metadata": {},
   "source": [
    "### 2.4 Build the Index\n",
    "\n",
    "Now, we'll read the data from the `document_chunks` table and build the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from the document_chunks table\n",
    "df_chunks = session.table(\"DOCUMENT_CHUNKS\").to_pandas()\n",
    "\n",
    "# Convert the data into Nodes\n",
    "nodes = []\n",
    "for idx, row in df_chunks.iterrows():\n",
    "    node = Node(text=row['CHUNK_TEXT'], doc_id=str(row['ID']), extra_info={'chunk_id': row['CHUNK_ID']})\n",
    "    nodes.append(node)\n",
    "\n",
    "# Initialize the embedding model\n",
    "embed_model = CortexEmbedding(session=session)\n",
    "\n",
    "# Create the vector store index\n",
    "vector_store = SimpleVectorStore()\n",
    "index = GPTVectorStoreIndex(nodes, embed_model=embed_model, vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setting-up-cortex-llm",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Setting Up Cortex LLM\n",
    "\n",
    "### 3.1 Define a Custom LLM Class for Snowflake Cortex\n",
    "\n",
    "We'll define a custom LLM class that uses Snowflake Cortex's text generation capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-cortexllm-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.base import LLM\n",
    "\n",
    "class CortexLLM(LLM):\n",
    "    def __init__(self, session):\n",
    "        self.session = session\n",
    "\n",
    "    def generate(self, prompt: str, **kwargs) -> str:\n",
    "        df = self.session.create_dataframe([[prompt]], schema=[\"PROMPT\"])\n",
    "        result_df = df.select(\n",
    "            self.session.call_function(\"CORTEX_GENERATE_TEXT\", df[\"PROMPT\"]).alias(\"RESPONSE\")\n",
    "        )\n",
    "        result = result_df.collect()\n",
    "        return result[0][\"RESPONSE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "note-on-cortex-generate-text-function",
   "metadata": {},
   "source": [
    "**Note**: Ensure that the `CORTEX_GENERATE_TEXT` function is available in your Snowflake environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initialize-llm-predictor-and-service-context",
   "metadata": {},
   "source": [
    "### 3.2 Initialize LLM Predictor and Service Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initialize-llm-predictor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import LLMPredictor, ServiceContext\n",
    "\n",
    "# Initialize the language model predictor using the custom CortexLLM\n",
    "cortex_llm = CortexLLM(session=session)\n",
    "llm_predictor = LLMPredictor(llm=cortex_llm)\n",
    "\n",
    "# Create a service context for LlamaIndex\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=llm_predictor,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "querying-the-index",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Querying the Index\n",
    "\n",
    "### 4.1 Raw Input vs. RAG Output\n",
    "\n",
    "#### Raw Input Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raw-input-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw input without RAG\n",
    "raw_question = \"What is Snowflake?\"\n",
    "print(\"Raw Question:\", raw_question)\n",
    "\n",
    "# Attempt to get a response without context\n",
    "try:\n",
    "    raw_response = cortex_llm.generate(raw_question)\n",
    "    print(\"Raw Response:\", raw_response)\n",
    "except Exception as e:\n",
    "    print(\"Error generating response:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-output-example",
   "metadata": {},
   "source": [
    "#### RAG Output Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag-output-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query using the RAG pipeline\n",
    "response = index.query(raw_question, service_context=service_context)\n",
    "print(\"RAG Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "### 4.2 Comparison\n",
    "\n",
    "You should observe that the RAG response is more informative and accurate due to the context provided by the indexed documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cortex-use-cases",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Cortex Use Cases\n",
    "\n",
    "### Use Case 1: Knowledge Base Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "use-case-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How does Snowflake Cortex help with AI applications?\"\n",
    "response = index.query(question, service_context=service_context)\n",
    "print(\"Use Case 1 Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "use-case-2",
   "metadata": {},
   "source": [
    "### Use Case 2: Contextual Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "use-case-2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt = \"Summarize the key features of LlamaIndex.\"\n",
    "response = index.query(summary_prompt, service_context=service_context)\n",
    "print(\"Use Case 2 Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "use-case-3",
   "metadata": {},
   "source": [
    "### Use Case 3: Personalized Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "use-case-3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_prompt = \"What are the next steps for integrating AI capabilities in our data platform?\"\n",
    "response = index.query(recommendation_prompt, service_context=service_context)\n",
    "print(\"Use Case 3 Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "\n",
    "- Preprocess documents using Snowflake Cortex's `PARSE_DOCUMENT` and `SPLIT_TEXT_RECURSIVE_CHARACTER` functions.\n",
    "- Use the active Snowflake session within a native notebook.\n",
    "- Build an index over the data using LlamaIndex with custom embedding and LLM classes that leverage Snowflake Cortex.\n",
    "- Set up a RAG pipeline to handle queries using Snowflake Cortex.\n",
    "- Compare raw input responses with RAG-augmented responses.\n",
    "- Explore different use cases for Cortex within the RAG pipeline.\n",
    "\n",
    "By leveraging these tools, you can build powerful AI applications that interact with your data stored in Snowflake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Expand Your Data**: Incorporate more extensive and diverse datasets.\n",
    "- **Enhance the Model**: Experiment with different Cortex models or parameters.\n",
    "- **Deploy Applications**: Integrate this pipeline into applications or dashboards for end-users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "notes",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Notes**:\n",
    "\n",
    "- Ensure that you have the necessary permissions to access Cortex functions in your Snowflake account.\n",
    "- Replace placeholder values like `'s3://your-bucket/document1.pdf'` with your actual file URLs.\n",
    "- The implementation details may vary depending on your Snowflake Cortex setup.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appendix-additional-information",
   "metadata": {},
   "source": [
    "# Appendix: Additional Information\n",
    "\n",
    "## Snowflake Cortex Functions\n",
    "\n",
    "### PARSE_DOCUMENT\n",
    "\n",
    "Extracts text from documents (e.g., PDFs) and preserves layout.\n",
    "\n",
    "**Syntax**:\n",
    "\n",
    "```sql\n",
    "PARSE_DOCUMENT(\n",
    "    FILE => '<file_url>',\n",
    "    FILE_TYPE => '<file_type>',\n",
    "    CONTENT => 'TEXT',\n",
    "    PARSE_STRATEGY => 'LAYOUT' | 'OCR'\n",
    ")\n",
    "```\n",
    "\n",
    "### SPLIT_TEXT_RECURSIVE_CHARACTER\n",
    "\n",
    "Splits text into chunks based on character count and overlap.\n",
    "\n",
    "**Syntax**:\n",
    "\n",
    "```sql\n",
    "SPLIT_TEXT_RECURSIVE_CHARACTER(\n",
    "    TEXT => '<text>',\n",
    "    CHUNK_LENGTH => <int>,\n",
    "    CHUNK_OVERLAP => <int>\n",
    ")\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llamaindex-overview",
   "metadata": {},
   "source": [
    "## LlamaIndex Overview\n",
    "\n",
    "LlamaIndex is a library that facilitates the creation of indices over your data to be used with large language models. It allows for efficient querying and retrieval of relevant information, which is essential for building RAG pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-remarks",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Final Remarks\n",
    "\n",
    "By completing this tutorial, you have learned how to build a Retrieval-Augmented Generation pipeline within Snowflake's native notebook environment using LlamaIndex and Snowflake Cortex's new SQL functions. This powerful combination enables you to create intelligent applications that can interact with your data securely and efficiently.\n",
    "\n",
    "Feel free to explore further and customize the pipeline to suit your specific needs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Snowpark)",
   "language": "python",
   "name": "python3_snowpark"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}